{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82b8ff40",
      "metadata": {
        "heading_collapsed": true,
        "id": "82b8ff40"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ebaa491",
      "metadata": {
        "hidden": true,
        "id": "6ebaa491"
      },
      "outputs": [],
      "source": [
        "#Import the necessary libraries\n",
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a08c4a6a",
      "metadata": {
        "heading_collapsed": true,
        "id": "a08c4a6a"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc05f12e",
      "metadata": {
        "hidden": true,
        "id": "dc05f12e"
      },
      "outputs": [],
      "source": [
        "#Load the pre-trained YOLOv3 weights and configuration files for object detection\n",
        "net = cv2.dnn.readNetFromDarknet(\"yolov3.cfg\",\"yolov3.weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31205f0e",
      "metadata": {
        "heading_collapsed": true,
        "id": "31205f0e"
      },
      "source": [
        "# Load COCO Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cfd6714",
      "metadata": {
        "hidden": true,
        "id": "1cfd6714"
      },
      "outputs": [],
      "source": [
        "#Load the COCO class labels:\n",
        "classes = []\n",
        "with open(\"coco.names\", \"r\") as f:\n",
        "    classes = [line.strip() for line in f.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0237a81b",
      "metadata": {
        "heading_collapsed": true,
        "id": "0237a81b"
      },
      "source": [
        "# Define Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a92a232",
      "metadata": {
        "hidden": true,
        "id": "5a92a232"
      },
      "outputs": [],
      "source": [
        "#Define the minimum distance threshold for social distancing (in pixels) and set up a scale factor for resizing the image\n",
        "min_distance = 120\n",
        "scale = 0.00392"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "867bd223",
      "metadata": {
        "heading_collapsed": true,
        "id": "867bd223"
      },
      "source": [
        "# Load Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023b8da8",
      "metadata": {
        "hidden": true,
        "id": "023b8da8"
      },
      "outputs": [],
      "source": [
        "#Load the video or use the webcam:\n",
        "ved_path = r'Pedestrian 480p.mp4'\n",
        "cap = cv2.VideoCapture(ved_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab62495",
      "metadata": {
        "heading_collapsed": true,
        "id": "3ab62495"
      },
      "source": [
        "# Social Distancing Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae4f51c8",
      "metadata": {
        "hidden": true,
        "id": "ae4f51c8"
      },
      "outputs": [],
      "source": [
        "#Process each frame in a loop:\n",
        "while True:\n",
        "    ret, image = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    height, width, _ = image.shape\n",
        "    # Perform object detection using YOLOv3\n",
        "    blob = cv2.dnn.blobFromImage(image, scale, (416, 416), (0, 0, 0), True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outs = net.forward(net.getUnconnectedOutLayersNames())\n",
        "\n",
        "    # Initialize lists for storing person centroids and bounding boxes\n",
        "    class_ids = []\n",
        "    confidences = []\n",
        "    boxes = []\n",
        "    centroids = []\n",
        "\n",
        "    for out in outs:\n",
        "        for detection in out:\n",
        "            scores = detection[5:]\n",
        "            class_id = np.argmax(scores)\n",
        "            confidence = scores[class_id]\n",
        "\n",
        "            if class_id == 0 and confidence > 0.5:\n",
        "                # Object is a person\n",
        "                center_x = int(detection[0] * width)\n",
        "                center_y = int(detection[1] * height)\n",
        "                w = int(detection[2] * width)\n",
        "                h = int(detection[3] * height)\n",
        "\n",
        "                # Calculate the bounding box coordinates\n",
        "                x = int(center_x - w / 2)\n",
        "                y = int(center_y - h / 2)\n",
        "\n",
        "                # Store the person's centroid and bounding box\n",
        "                centroids.append((center_x, center_y))\n",
        "                boxes.append([x, y, w, h])\n",
        "                confidences.append(float(confidence))\n",
        "                class_ids.append(class_id)\n",
        "\n",
        "    # Perform non-maximum suppression to eliminate redundant overlapping bounding boxes\n",
        "    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
        "\n",
        "    # Check the distance between each pair of centroids\n",
        "    for i in range(len(centroids)):\n",
        "        if i in indices:\n",
        "            x1, y1 = centroids[i]\n",
        "            for j in range(i + 1, len(centroids)):\n",
        "                if j in indices:\n",
        "                    x2, y2 = centroids[j]\n",
        "                    distance = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
        "\n",
        "                    # If distance is less than the threshold, draw a red bounding box\n",
        "                    if distance < min_distance:\n",
        "                        cv2.rectangle(image, (boxes[i][0], boxes[i][1]), (boxes[i][0] + boxes[i][2], boxes[i][1] + boxes[i][3]), (0, 0, 255), 2)\n",
        "                        cv2.rectangle(image, (boxes[j][0], boxes[j][1]), (boxes[j][0] + boxes[j][2], boxes[j][1] + boxes[j][3]), (0, 0, 255), 2)\n",
        "                    # If distance is larger than the threshold, draw a green bounding box\n",
        "                    else:\n",
        "                        cv2.rectangle(image, (boxes[i][0], boxes[i][1]), (boxes[i][0] + boxes[i][2], boxes[i][1] + boxes[i][3]), (0, 255, 0), 2)\n",
        "                        cv2.rectangle(image, (boxes[j][0], boxes[j][1]), (boxes[j][0] + boxes[j][2], boxes[j][1] + boxes[j][3]), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the resulting image\n",
        "    cv2.imshow(\"Social Distancing Detector\", image)\n",
        "    if cv2.waitKey(1) == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the capture and close the windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "82b8ff40",
        "a08c4a6a",
        "31205f0e",
        "0237a81b",
        "867bd223",
        "3ab62495"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}